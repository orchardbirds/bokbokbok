{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Focal Loss?\n",
    "\n",
    "Focal Loss addresses class imbalance in tasks such as object detection. Focal loss applies a modulating term to the Cross Entropy loss in order to focus learning on hard negative examples. It is a dynamically scaled Cross Entropy loss, where the scaling factor decays to zero as confidence in the correct class increases. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. This scaling factor is *gamma*. The more *gamma* is increased, the more the model is focussed on the hard, misclassified examples.\n",
    "\n",
    "We employ Weighted Focal Loss, which further allows us to reduce false positives or false negatives depending on our value of *alpha*:\n",
    "\n",
    "A value *alpha* > 1 decreases the false negative count, hence increasing the recall. Conversely, setting *alpha* < 1 decreases the false positive count and increases the precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bokbokbok.loss_functions.classification import WeightedFocalLoss\n",
    "from bokbokbok.eval_metrics.classification import WeightedFocalMetric\n",
    "from bokbokbok.utils import clip_sigmoid\n",
    "\n",
    "X, y = make_classification(n_samples=1000, \n",
    "                           n_features=10, \n",
    "                           random_state=41114)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, \n",
    "                                                      y, \n",
    "                                                      test_size=0.25, \n",
    "                                                      random_state=41114)\n",
    "\n",
    "alpha = 0.7  # Reduce False Positives\n",
    "gamma = 2    # Focus on misclassified examples more strictly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage in LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train = lgb.Dataset(X_train, y_train)\n",
    "valid = lgb.Dataset(X_valid, y_valid, reference=train)\n",
    "params = {\n",
    "     'n_estimators': 300,\n",
    "     'seed': 41114,\n",
    "     'n_jobs': 8,\n",
    "     'learning_rate': 0.1,\n",
    "     'early_stopping_rounds': 100,\n",
    "     'objective': WeightedFocalLoss(alpha=alpha, gamma=gamma)\n",
    "   }\n",
    "\n",
    "clf = lgb.train(params=params,\n",
    "                train_set=train,\n",
    "                valid_sets=[train, valid],\n",
    "                valid_names=['train','valid'],\n",
    "                feval=WeightedFocalMetric(alpha=alpha, gamma=gamma)\n",
    "                )\n",
    "\n",
    "roc_auc_score(y_valid, clip_sigmoid(clf.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage in XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "\n",
    "params = {\n",
    "     'seed': 41114,\n",
    "     'learning_rate': 0.1,\n",
    "    'disable_default_eval_metric': 1\n",
    "   }\n",
    "\n",
    "bst = xgb.train(params,\n",
    "          dtrain=dtrain,\n",
    "          num_boost_round=300,\n",
    "          early_stopping_rounds=10,\n",
    "          verbose_eval=10,\n",
    "          obj=WeightedFocalLoss(alpha=alpha, gamma=gamma),\n",
    "          maximize=False,\n",
    "          feval=WeightedFocalMetric(alpha=alpha, gamma=gamma, XGBoost=True),\n",
    "          evals=[(dtrain, 'dtrain'), (dvalid, 'dvalid')])\n",
    "\n",
    "roc_auc_score(y_valid, clip_sigmoid(bst.predict(dvalid)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:skorecard_py37]",
   "language": "python",
   "name": "conda-env-skorecard_py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
