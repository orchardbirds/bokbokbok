{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Log Cosh Loss?\n",
    "\n",
    "Log Cosh Loss addresses the small number of problems that can arise from using Mean Absolute Error due to its sharpness. Log(cosh(x)) is a way to very closely approximate Mean Absolute Error while retaining a 'smooth' function.\n",
    "\n",
    "Do note that large y-values can cause issues here, which is why the y-values are scaled below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from bokbokbok.eval_metrics.regression import LogCoshMetric\n",
    "from bokbokbok.loss_functions.regression import LogCoshLoss\n",
    "\n",
    "X, y = make_regression(n_samples=1000, \n",
    "                       n_features=10, \n",
    "                       random_state=41114)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, \n",
    "                                                      y/100, \n",
    "                                                      test_size=0.25, \n",
    "                                                      random_state=41114)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage in LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train = lgb.Dataset(X_train, y_train)\n",
    "valid = lgb.Dataset(X_valid, y_valid, reference=train)\n",
    "params = {\n",
    "     'n_estimators': 3000,\n",
    "     'seed': 41114,\n",
    "     'n_jobs': 8,\n",
    "     'learning_rate': 0.1,\n",
    "     'verbose': 100,\n",
    "   }\n",
    "\n",
    "clf = lgb.train(params=params,\n",
    "                train_set=train,\n",
    "                valid_sets=[train, valid],\n",
    "                valid_names=['train','valid'],\n",
    "                fobj=LogCoshLoss(),\n",
    "                feval=LogCoshMetric(),\n",
    "                early_stopping_rounds=100,\n",
    "                verbose_eval=100)\n",
    "\n",
    "mean_absolute_error(y_valid, clf.predict(X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage in XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "\n",
    "params = {\n",
    "     'seed': 41114,\n",
    "     'learning_rate': 0.1,\n",
    "    'disable_default_eval_metric': 1\n",
    "   }\n",
    "\n",
    "bst = xgb.train(params,\n",
    "          dtrain=dtrain,\n",
    "          num_boost_round=3000,\n",
    "          early_stopping_rounds=10,\n",
    "          verbose_eval=100,\n",
    "          obj=LogCoshLoss(),\n",
    "          maximize=False,\n",
    "          feval=LogCoshMetric(XGBoost=True),\n",
    "          evals=[(dtrain, 'dtrain'), (dvalid, 'dvalid')])\n",
    "\n",
    "mean_absolute_error(y_valid, bst.predict(dvalid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:skorecard_py37]",
   "language": "python",
   "name": "conda-env-skorecard_py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
