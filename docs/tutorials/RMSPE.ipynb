{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use (Root Mean) Squared Percentage Error?\n",
    "\n",
    "This function is defined according to [this Kaggle competition](https://www.kaggle.com/c/optiver-realized-volatility-prediction/overview/evaluation) for volatility calculation. \n",
    "\n",
    "RMSPE cannot be used as a Loss function - the gradient is constant and hence the Hessian is 0. Nevertheless, it can still be used as an evaluation metric as the model trains. To use the loss function, we simply remove the square for a non-zero Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from bokbokbok.eval_metrics.regression import RMSPEMetric\n",
    "from bokbokbok.loss_functions.regression import SPELoss\n",
    "\n",
    "X, y = make_regression(n_samples=10000, \n",
    "                       n_features=10, \n",
    "                       random_state=41114)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, \n",
    "                                                      y, \n",
    "                                                      test_size=0.25, \n",
    "                                                      random_state=41114)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage in LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train = lgb.Dataset(X_train, y_train)\n",
    "valid = lgb.Dataset(X_valid, y_valid, reference=train)\n",
    "params = {\n",
    "     'n_estimators': 3000,\n",
    "     'seed': 41114,\n",
    "     'n_jobs': 8,\n",
    "     'max_leaves':10,\n",
    "     'early_stopping_rounds': 100,\n",
    "     'objective': SPELoss(),\n",
    "   }\n",
    "\n",
    "clf = lgb.train(params=params,\n",
    "                train_set=train,\n",
    "                valid_sets=[train, valid],\n",
    "                valid_names=['train','valid'],\n",
    "                feval=RMSPEMetric())\n",
    "\n",
    "mean_absolute_error(y_valid, clf.predict(X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage in XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "\n",
    "params = {\n",
    "     'seed': 41114,\n",
    "     'learning_rate': 0.1,\n",
    "    'disable_default_eval_metric': 1\n",
    "   }\n",
    "\n",
    "bst = xgb.train(params,\n",
    "          dtrain=dtrain,\n",
    "          num_boost_round=3000,\n",
    "          early_stopping_rounds=100,\n",
    "          verbose_eval=100,\n",
    "          obj=SPELoss(),\n",
    "          maximize=False,\n",
    "          feval=RMSPEMetric(XGBoost=True),\n",
    "          evals=[(dtrain, 'dtrain'), (dvalid, 'dvalid')])\n",
    "\n",
    "mean_absolute_error(y_valid, bst.predict(dvalid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:skorecard_py37]",
   "language": "python",
   "name": "conda-env-skorecard_py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
