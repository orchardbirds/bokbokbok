{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to the bokbokbok doks!","text":"<p>bokbokbok is a Python library that lets us easily implement custom loss functions and eval metrics in LightGBM and XGBoost.</p>"},{"location":"index.html#example-usage-weighted-cross-entropy","title":"Example Usage - Weighted Cross Entropy","text":"<pre><code>params = {\"objective\": WeightedCrossEntropyLoss(alpha=alpha)}\nclf = lgb.train(params=params,\n                train_set=train,\n                valid_sets=[train, valid],\n                valid_names=['train','valid'],\n                feval=WeightedCrossEntropyMetric(alpha=alpha))\n</code></pre>"},{"location":"index.html#licence","title":"Licence","text":"<p>bokbokbok is created under the MIT License, see more in the LICENSE file</p>"},{"location":"derivations/focal.html","title":"Focal Loss","text":""},{"location":"derivations/focal.html#weighted-focal-loss","title":"Weighted Focal Loss","text":"<p>Weighted Focal Loss applies a scaling parameter alpha and a focusing parameter gamma to Binary Cross Entropy</p> <p>We take the definition of the Focal Loss from this paper:</p> <p></p> <p>where:</p> <p></p> <p>This is equivalent to writing:</p> <p></p> <p>We calculate the Gradient:</p> <p></p> <p>We also need to calculate the Hessian:</p> <p></p> <p>By setting alpha = 1 and gamma = 0 we obtain the Gradient and Hessian for Binary Cross Entropy Loss, as expected.</p>"},{"location":"derivations/log_cosh.html","title":"Log Cosh Error","text":""},{"location":"derivations/log_cosh.html#log-cosh-error","title":"Log Cosh Error","text":"<p>The equation for Log Cosh Error is:</p> <p></p> <p>We calculate the Gradient:</p> <p></p> <p>We also need to calculate the Hessian:</p> <p></p>"},{"location":"derivations/note.html","title":"A Note About Gradients in Classification Problems","text":"<p>For the gradient boosting packages we have to calculate the gradient of the Loss function with respect to the marginal probabilites.</p> <p>In this case, we must calculate</p> <p></p> <p>The Hessian is similarly calculated:</p> <p></p> <p>Where y-hat is the sigmoid function, unless stated otherwise:</p> <p></p> <p>We will make use of the following property for the calculations of the Gradients and Hessians:</p> <p></p> <p>Note to avoid divide-by-zero errors, we clip the values of the sigmoid such that the output of the sigmoid is bound by 10<sup>-15</sup> from below and 1 - 10<sup>-15</sup> from above.</p>"},{"location":"derivations/wce.html","title":"Weighted Cross Entropy","text":""},{"location":"derivations/wce.html#weighted-cross-entropy-loss","title":"Weighted Cross Entropy Loss","text":"<p>Weighted Cross Entropy applies a scaling parameter alpha  to Binary Cross Entropy, allowing us to penalise false positives or false negatives more harshly. If you want false positives to be penalised more than false negatives, alpha must be greater than 1. Otherwise, it must be less than 1. </p> <p>The equations for Binary and Weighted Cross Entropy Loss are the following:</p> <p></p> <p></p> <p>We calculate the Gradient:</p> <p></p> <p>We also need to calculate the Hessian:</p> <p></p> <p>By setting alpha = 1 we obtain the Gradient and Hessian for Binary Cross Entropy Loss, as expected.</p>"},{"location":"getting_started/install.html","title":"Installation","text":"<p>In order to install bokbokbok you need to use Python 3.7 or higher.</p> <p>Install <code>bokbokbok</code> via pip with:</p> <pre><code>pip install bokbokbok\n</code></pre> <p>Alternatively you can fork/clone and run:</p> <pre><code>git clone https://gitlab.com/orchardbirds/bokbokbok.git\ncd bokbokbok\npoetry install .\n</code></pre>"},{"location":"reference/eval_metrics_binary.html","title":"bokbokbok.eval_metrics.binary_classification","text":""},{"location":"reference/eval_metrics_binary.html#bokbokbok.eval_metrics.classification.binary_eval_metrics.F1_Score_Binary","title":"<code>F1_Score_Binary(XGBoost=False, *args, **kwargs)</code>","text":"<p>Implements the f1_score metric from scikit learn</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>The arguments to be fed into the scikit learn metric.</p> <code>()</code> <code>XGBoost</code> <code>Bool</code> <p>Set to True if using XGBoost. We assume LightGBM as default use.             Note that you should also set <code>maximize=True</code> in the XGBoost train function</p> <code>False</code> Source code in <code>bokbokbok/eval_metrics/classification/binary_eval_metrics.py</code> <pre><code>def F1_Score_Binary(\n    XGBoost: bool = False,\n    *args: Any, \n    **kwargs: Any,\n    ) -&gt; Callable:\n    \"\"\"\n    Implements the f1_score metric\n    [from scikit learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn-metrics-f1-score)\n\n    Args:\n        *args: The arguments to be fed into the scikit learn metric.\n        XGBoost (Bool): Set to True if using XGBoost. We assume LightGBM as default use.\n                        Note that you should also set `maximize=True` in the XGBoost train function\n\n    \"\"\"\n    def binary_f1_score(\n        yhat: np.ndarray, \n        data: \"xgb.DMatrix\", \n        XGBoost: bool = XGBoost\n        ) -&gt; Union[tuple[str, Any], tuple[str, Any, bool]]: # needs better typing for f1 but I don't care\n        \"\"\"\n        F1 Score.\n\n        Args:\n            yhat: Predictions\n            dtrain: The XGBoost / LightGBM dataset\n            XGBoost (Bool): If XGBoost is to be implemented\n\n        Returns:\n            Name of the eval metric, Eval score, Bool to maximise function\n        \"\"\"\n        y_true = data.get_label()\n        yhat = np.round(yhat)\n        if XGBoost:\n            return \"F1\", f1_score(y_true, yhat, *args, **kwargs)\n        else:\n            return \"F1\", f1_score(y_true, yhat, *args, **kwargs), True\n\n    return binary_f1_score\n</code></pre>"},{"location":"reference/eval_metrics_binary.html#bokbokbok.eval_metrics.classification.binary_eval_metrics.WeightedCrossEntropyMetric","title":"<code>WeightedCrossEntropyMetric(alpha=0.5, XGBoost=False)</code>","text":"<p>Calculates the Weighted Cross Entropy Metric by applying a weighting factor alpha, allowing one to trade off recall and precision by up- or down-weighting the cost of a positive error relative to a negative error.</p> <p>A value alpha &gt; 1 decreases the false negative count, hence increasing the recall. Conversely, setting alpha &lt; 1 decreases the false positive count and increases the precision. </p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>The scale to be applied.</p> <code>0.5</code> <code>XGBoost</code> <code>Bool</code> <p>Set to True if using XGBoost. We assume LightGBM as default use.             Note that you should also set <code>maximize=False</code> in the XGBoost train function</p> <code>False</code> Source code in <code>bokbokbok/eval_metrics/classification/binary_eval_metrics.py</code> <pre><code>def WeightedCrossEntropyMetric(\n    alpha: float = 0.5, \n    XGBoost: bool = False\n    ) -&gt; Callable:\n    \"\"\"\n    Calculates the Weighted Cross Entropy Metric by applying a weighting factor alpha, allowing one to\n    trade off recall and precision by up- or down-weighting the cost of a positive error relative to a\n    negative error.\n\n    A value alpha &gt; 1 decreases the false negative count, hence increasing the recall.\n    Conversely, setting alpha &lt; 1 decreases the false positive count and increases the precision. \n\n    Args:\n        alpha (float): The scale to be applied.\n        XGBoost (Bool): Set to True if using XGBoost. We assume LightGBM as default use.\n                        Note that you should also set `maximize=False` in the XGBoost train function\n\n    \"\"\"\n\n\n    def weighted_cross_entropy_metric(\n        yhat: np.ndarray, \n        dtrain: \"xgb.DMatrix\", \n        alpha=alpha, \n        XGBoost=XGBoost\n        ) -&gt; Union[tuple[str, float], tuple[str, float, bool]]:\n        \"\"\"\n        Weighted Cross Entropy Metric.\n\n        Args:\n            yhat: Predictions\n            dtrain: The XGBoost / LightGBM dataset\n            alpha (float): Scale applied\n            XGBoost (Bool): If XGBoost is to be implemented\n\n        Returns:\n            Name of the eval metric, Eval score, Bool to minimise function\n\n        \"\"\"\n        y = dtrain.get_label()\n        yhat = clip_sigmoid(yhat)\n        elements = - alpha * y * np.log(yhat) - (1 - y) * np.log(1 - yhat)\n        if XGBoost:\n            return f\"WCE_alpha{alpha}\", (np.sum(elements) / len(y))\n        else:\n            return f\"WCE_alpha{alpha}\", (np.sum(elements) / len(y)), False\n\n    return weighted_cross_entropy_metric\n</code></pre>"},{"location":"reference/eval_metrics_binary.html#bokbokbok.eval_metrics.classification.binary_eval_metrics.WeightedFocalMetric","title":"<code>WeightedFocalMetric(alpha=1.0, gamma=2.0, XGBoost=False)</code>","text":"<p>Implements alpha-weighted Focal Loss</p> <p>The more gamma is increased, the more the model is focussed on the hard, misclassified examples.</p> <p>A value alpha &gt; 1 decreases the false negative count, hence increasing the recall. Conversely, setting alpha &lt; 1 decreases the false positive count and increases the precision. </p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>The scale to be applied.</p> <code>1.0</code> <code>gamma</code> <code>float</code> <p>The focusing parameter to be applied</p> <code>2.0</code> <code>XGBoost</code> <code>Bool</code> <p>Set to True if using XGBoost. We assume LightGBM as default use.             Note that you should also set <code>maximize=False</code> in the XGBoost train function</p> <code>False</code> Source code in <code>bokbokbok/eval_metrics/classification/binary_eval_metrics.py</code> <pre><code>def WeightedFocalMetric(\n    alpha: float = 1.0, \n    gamma: float = 2.0, \n    XGBoost: bool = False\n    ) -&gt; Callable:\n    \"\"\"\n    Implements [alpha-weighted Focal Loss](https://arxiv.org/pdf/1708.02002.pdf)\n\n    The more gamma is increased, the more the model is focussed on the hard, misclassified examples.\n\n    A value alpha &gt; 1 decreases the false negative count, hence increasing the recall.\n    Conversely, setting alpha &lt; 1 decreases the false positive count and increases the precision. \n\n    Args:\n        alpha (float): The scale to be applied.\n        gamma (float): The focusing parameter to be applied\n        XGBoost (Bool): Set to True if using XGBoost. We assume LightGBM as default use.\n                        Note that you should also set `maximize=False` in the XGBoost train function\n    \"\"\"\n\n    def focal_metric(\n        yhat: np.ndarray, \n        dtrain: \"xgb.DMatrix\", \n        alpha: float = alpha, \n        gamma: float = gamma, \n        XGBoost: bool = XGBoost) -&gt; Union[tuple[str, float], tuple[str, float, bool]]:\n        \"\"\"\n        Weighted Focal Loss Metric.\n\n        Args:\n            yhat: Predictions\n            dtrain: The XGBoost / LightGBM dataset\n            alpha (float): Scale applied\n            gamma (float): Focusing parameter\n            XGBoost (Bool): If XGBoost is to be implemented\n\n        Returns:\n            Name of the eval metric, Eval score, Bool to minimise function\n\n        \"\"\"\n        y = dtrain.get_label()\n        yhat = clip_sigmoid(yhat)\n\n        elements = (- alpha * y * np.log(yhat) * np.power(1 - yhat, gamma) -\n                    (1 - y) * np.log(1 - yhat) * np.power(yhat, gamma))\n\n        if XGBoost:\n            return f'Focal_alpha{alpha}_gamma{gamma}', (np.sum(elements) / len(y))\n        else:\n            return f'Focal_alpha{alpha}_gamma{gamma}', (np.sum(elements) / len(y)), False\n\n    return focal_metric\n</code></pre>"},{"location":"reference/eval_metrics_multiclass.html","title":"bokbokbok.eval_metrics.multiclass_classification","text":""},{"location":"reference/eval_metrics_multiclass.html#bokbokbok.eval_metrics.classification.multiclass_eval_metrics.QuadraticWeightedKappaMetric","title":"<code>QuadraticWeightedKappaMetric(XGBoost=False)</code>","text":"<p>Calculates the Weighted Cross Entropy Metric by applying a weighting factor alpha, allowing one to trade off recall and precision by up- or down-weighting the cost of a positive error relative to a negative error.</p> <p>A value alpha &gt; 1 decreases the false negative count, hence increasing the recall. Conversely, setting alpha &lt; 1 decreases the false positive count and increases the precision. </p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>The scale to be applied.</p> required <code>XGBoost</code> <code>Bool</code> <p>Set to True if using XGBoost. We assume LightGBM as default use.             Note that you should also set <code>maximize=False</code> in the XGBoost train function</p> <code>False</code> Source code in <code>bokbokbok/eval_metrics/classification/multiclass_eval_metrics.py</code> <pre><code>def QuadraticWeightedKappaMetric(XGBoost: bool = False) -&gt; Callable:\n    \"\"\"\n    Calculates the Weighted Cross Entropy Metric by applying a weighting factor alpha, allowing one to\n    trade off recall and precision by up- or down-weighting the cost of a positive error relative to a\n    negative error.\n\n    A value alpha &gt; 1 decreases the false negative count, hence increasing the recall.\n    Conversely, setting alpha &lt; 1 decreases the false positive count and increases the precision. \n\n    Args:\n        alpha (float): The scale to be applied.\n        XGBoost (Bool): Set to True if using XGBoost. We assume LightGBM as default use.\n                        Note that you should also set `maximize=False` in the XGBoost train function\n\n    \"\"\"\n\n\n    def quadratic_weighted_kappa_metric(\n        yhat: np.ndarray,\n        dtrain: \"xgb.DMatrix\", \n        XGBoost: bool = XGBoost) -&gt; Union[tuple[str, float], tuple[str, float, bool]]:\n        \"\"\"\n        Weighted Cross Entropy Metric.\n\n        Args:\n            yhat: Predictions\n            dtrain: The XGBoost / LightGBM dataset\n            XGBoost (Bool): If XGBoost is to be implemented\n\n        Returns:\n            Name of the eval metric, Eval score, Bool to maximise function\n\n        \"\"\"\n        y = dtrain.get_label()\n        num_class = len(np.unique(dtrain.get_label()))\n\n        if not XGBoost:\n            # LightGBM needs extra reshaping\n            yhat = yhat.reshape(num_class, len(y)).T\n        yhat = yhat.argmax(axis=1)\n\n        qwk = cohen_kappa_score(y, yhat, weights=\"quadratic\")\n\n        if XGBoost:\n            return \"QWK\", qwk\n        else:\n            return \"QWK\", qwk, True\n\n    return quadratic_weighted_kappa_metric\n</code></pre>"},{"location":"reference/eval_metrics_regression.html","title":"bokbokbok.eval_metrics.regression","text":""},{"location":"reference/eval_metrics_regression.html#bokbokbok.eval_metrics.regression.regression_eval_metrics.LogCoshMetric","title":"<code>LogCoshMetric(XGBoost=False)</code>","text":"<p>Calculates the Log Cosh Error as an alternative to Mean Absolute Error. Args:     XGBoost (Bool): Set to True if using XGBoost. We assume LightGBM as default use.                     Note that you should also set <code>maximize=False</code> in the XGBoost train function</p> Source code in <code>bokbokbok/eval_metrics/regression/regression_eval_metrics.py</code> <pre><code>def LogCoshMetric(XGBoost: bool = False) -&gt; Callable:\n    \"\"\"\n    Calculates the [Log Cosh Error](https://openreview.net/pdf?id=rkglvsC9Ym) as an alternative to\n    Mean Absolute Error.\n    Args:\n        XGBoost (Bool): Set to True if using XGBoost. We assume LightGBM as default use.\n                        Note that you should also set `maximize=False` in the XGBoost train function\n\n    \"\"\"\n    def log_cosh_error(\n        yhat: np.ndarray, \n        dtrain: \"xgb.DMatrix\", \n        XGBoost: bool = XGBoost\n        ) -&gt; Union[tuple[str, float], tuple[str, float, bool]]:\n        \"\"\"\n        Root Mean Squared Log Error.\n        All input labels are required to be greater than -1.\n\n        yhat: Predictions\n        dtrain: The XGBoost / LightGBM dataset\n        XGBoost (Bool): If XGBoost is to be implemented\n        \"\"\"\n\n        y = dtrain.get_label()\n        elements = np.log(np.cosh(yhat - y))\n        if XGBoost:\n            return \"LogCosh\", float(np.sum(elements) / len(y))\n        else:\n            return \"LogCosh\", float(np.sum(elements) / len(y)), False\n\n    return log_cosh_error\n</code></pre>"},{"location":"reference/eval_metrics_regression.html#bokbokbok.eval_metrics.regression.regression_eval_metrics.RMSPEMetric","title":"<code>RMSPEMetric(XGBoost=False)</code>","text":"<p>Calculates the Root Mean Squared Percentage Error: https://www.kaggle.com/c/optiver-realized-volatility-prediction/overview/evaluation</p> <p>The corresponding Loss function is Squared Percentage Error. Args:     XGBoost (Bool): Set to True if using XGBoost. We assume LightGBM as default use.                     Note that you should also set <code>maximize=False</code> in the XGBoost train function</p> Source code in <code>bokbokbok/eval_metrics/regression/regression_eval_metrics.py</code> <pre><code>def RMSPEMetric(XGBoost: bool = False) -&gt; Callable:\n    \"\"\"\n    Calculates the Root Mean Squared Percentage Error:\n    https://www.kaggle.com/c/optiver-realized-volatility-prediction/overview/evaluation\n\n    The corresponding Loss function is Squared Percentage Error.\n    Args:\n        XGBoost (Bool): Set to True if using XGBoost. We assume LightGBM as default use.\n                        Note that you should also set `maximize=False` in the XGBoost train function\n\n    \"\"\"\n    def RMSPE(\n        yhat: np.ndarray, \n        dtrain: \"xgb.DMatrix\", \n        XGBoost: bool = XGBoost) -&gt; Union[tuple[str, float], tuple[str, float, bool]]:\n        \"\"\"\n        Root Mean Squared Log Error.\n        All input labels are required to be greater than -1.\n\n        yhat: Predictions\n        dtrain: The XGBoost / LightGBM dataset\n        XGBoost (Bool): If XGBoost is to be implemented\n        \"\"\"\n\n        y = dtrain.get_label()\n        elements = ((y - yhat) / y) ** 2\n        if XGBoost:\n            return \"RMSPE\", float(np.sqrt(np.sum(elements) / len(y)))\n        else:\n            return \"RMSPE\", float(np.sqrt(np.sum(elements) / len(y))), False\n\n    return RMSPE\n</code></pre>"},{"location":"reference/loss_functions_classification.html","title":"bokbokbok.loss_functions.classification","text":""},{"location":"reference/loss_functions_classification.html#bokbokbok.loss_functions.classification.classification_loss_functions.WeightedCrossEntropyLoss","title":"<code>WeightedCrossEntropyLoss(alpha=0.5)</code>","text":"<p>Calculates the Weighted Cross-Entropy Loss, which applies a factor alpha, allowing one to trade off recall and precision by up- or down-weighting the cost of a positive error relative to a negative error.</p> <p>A value alpha &gt; 1 decreases the false negative count, hence increasing the recall. Conversely, setting alpha &lt; 1 decreases the false positive count and increases the precision.</p> Source code in <code>bokbokbok/loss_functions/classification/classification_loss_functions.py</code> <pre><code>def WeightedCrossEntropyLoss(alpha: float = 0.5) -&gt; Callable:\n    \"\"\"\n    Calculates the Weighted Cross-Entropy Loss, which applies a factor alpha, allowing one to\n    trade off recall and precision by up- or down-weighting the cost of a positive error relative\n    to a negative error.\n\n    A value alpha &gt; 1 decreases the false negative count, hence increasing the recall.\n    Conversely, setting alpha &lt; 1 decreases the false positive count and increases the precision. \n    \"\"\"\n\n    def _gradient(\n        yhat: np.ndarray, \n        dtrain: \"xgb.DMatrix\", \n        alpha: float) -&gt; np.ndarray:\n        \"\"\"Compute the weighted cross-entropy gradient.\n\n        Args:\n            yhat (np.array): Margin predictions\n            dtrain: The XGBoost / LightGBM dataset\n            alpha (float): Scale applied\n\n        Returns:\n            grad: Weighted cross-entropy gradient\n        \"\"\"\n        y = dtrain.get_label()\n\n        yhat = clip_sigmoid(yhat)\n\n        grad = (y * yhat * (alpha - 1)) + yhat - (alpha * y)\n\n        return grad\n\n    def _hessian(yhat: np.ndarray, dtrain: \"xgb.DMatrix\", alpha: float) -&gt; np.ndarray:\n        \"\"\"Compute the weighted cross-entropy hessian.\n\n        Args:\n            yhat (np.array): Margin predictions\n            dtrain: The XGBoost / LightGBM dataset\n            alpha (float): Scale applied\n\n        Returns:\n            hess: Weighted cross-entropy Hessian\n        \"\"\"\n        y = dtrain.get_label()\n        yhat = clip_sigmoid(yhat)\n\n        hess = (y * (alpha - 1) + 1) * yhat * (1 - yhat)\n\n        return hess\n\n    def weighted_cross_entropy(\n            yhat: np.ndarray,\n            dtrain: \"xgb.DMatrix\",\n            alpha: float = alpha\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Calculate gradient and hessian for weight cross-entropy,\n\n        Args:\n            yhat (np.array): Predictions\n            dtrain: The XGBoost / LightGBM dataset\n            alpha (float): Scale applied\n\n        Returns:\n            grad: Weighted cross-entropy gradient\n            hess: Weighted cross-entropy Hessian\n        \"\"\"\n        grad = _gradient(yhat, dtrain, alpha=alpha)\n\n        hess = _hessian(yhat, dtrain, alpha=alpha)\n\n        return grad, hess\n\n    return weighted_cross_entropy\n</code></pre>"},{"location":"reference/loss_functions_classification.html#bokbokbok.loss_functions.classification.classification_loss_functions.WeightedFocalLoss","title":"<code>WeightedFocalLoss(alpha=1.0, gamma=2.0)</code>","text":"<p>Calculates the Weighted Focal Loss.</p> <p>Note that if using alpha = 1 and gamma = 0, this is the same as using regular Cross Entropy.</p> <p>The more gamma is increased, the more the model is focussed on the hard, misclassified examples.</p> <p>A value alpha &gt; 1 decreases the false negative count, hence increasing the recall. Conversely, setting alpha &lt; 1 decreases the false positive count and increases the precision.</p> Source code in <code>bokbokbok/loss_functions/classification/classification_loss_functions.py</code> <pre><code>def WeightedFocalLoss(alpha: float = 1.0, gamma: float = 2.0) -&gt; Callable:\n    \"\"\"\n    Calculates the [Weighted Focal Loss.](https://arxiv.org/pdf/1708.02002.pdf)\n\n    Note that if using alpha = 1 and gamma = 0,\n    this is the same as using regular Cross Entropy.\n\n    The more gamma is increased, the more the model is focussed on the hard, misclassified examples.\n\n    A value alpha &gt; 1 decreases the false negative count, hence increasing the recall.\n    Conversely, setting alpha &lt; 1 decreases the false positive count and increases the precision. \n\n    \"\"\"\n\n    def _gradient(yhat: np.ndarray, dtrain: \"xgb.DMatrix\", alpha: float, gamma: float) -&gt; np.ndarray:\n        \"\"\"Compute the weighted focal gradient.\n\n        Args:\n            yhat (np.array): Margin predictions\n            dtrain: The XGBoost / LightGBM dataset\n            alpha (float): Scale applied\n            gamma (float): Focusing parameter\n\n        Returns:\n            grad: Weighted Focal Loss gradient\n        \"\"\"\n        y = dtrain.get_label()\n\n        yhat = clip_sigmoid(yhat)\n\n        grad = (\n                alpha * y * np.power(1 - yhat, gamma) * (gamma * yhat * np.log(yhat) + yhat - 1) +\n                (1 - y) * np.power(yhat, gamma) * (yhat - gamma * np.log(1 - yhat) * (1 - yhat))\n                )\n\n        return grad\n\n    def _hessian(yhat: np.ndarray, dtrain: \"xgb.DMatrix\", alpha: float, gamma: float) -&gt; np.ndarray:\n        \"\"\"Compute the weighted focal hessian.\n\n        Args:\n            yhat (np.array): Margin predictions\n            dtrain: The XGBoost / LightGBM dataset\n            alpha (float): Scale applied\n            gamma (float): Focusing parameter\n\n        Returns:\n            hess: Weighted Focal Loss Hessian\n        \"\"\"\n        y = dtrain.get_label()\n\n        yhat = clip_sigmoid(yhat)\n\n        hess = (\n                alpha * y * yhat * np.power(1 - y,\n                                            gamma) * (gamma * (1 - yhat) * np.log(yhat) + 2 * gamma * (1 - yhat) -\n                                                      np.power(gamma, 2) * yhat * np.log(yhat) + 1 - yhat) +\n                (1 - y) * np.power(yhat, gamma + 1) * (1 - yhat) * (2 * gamma + gamma * (np.log(1 - yhat)) + 1)\n                )\n\n        return hess\n\n    def focal_loss(\n            yhat: np.ndarray,\n            dtrain: \"xgb.DMatrix\",\n            alpha: float = alpha,\n            gamma: float = gamma) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Calculate gradient and hessian for Focal Loss,\n\n        Args:\n            yhat (np.array): Margin predictions\n            dtrain: The XGBoost / LightGBM dataset\n            alpha (float): Scale applied\n            gamma (float): Focusing parameter\n\n        Returns:\n            grad: Focal Loss gradient\n            hess: Focal Loss Hessian\n        \"\"\"\n\n        grad = _gradient(yhat, dtrain, alpha=alpha, gamma=gamma)\n\n        hess = _hessian(yhat, dtrain, alpha=alpha, gamma=gamma)\n\n        return grad, hess\n\n    return focal_loss\n</code></pre>"},{"location":"reference/loss_functions_regression.html","title":"bokbokbok.loss_functions.regression","text":""},{"location":"reference/loss_functions_regression.html#bokbokbok.loss_functions.regression.regression_loss_functions.LogCoshLoss","title":"<code>LogCoshLoss()</code>","text":"<p>Log Cosh Loss is an alternative to Mean Absolute Error.</p> Source code in <code>bokbokbok/loss_functions/regression/regression_loss_functions.py</code> <pre><code>def LogCoshLoss() -&gt; Callable:\n    \"\"\"\n    [Log Cosh Loss](https://openreview.net/pdf?id=rkglvsC9Ym) is an alternative to Mean Absolute Error.\n    \"\"\"\n\n    def _gradient(yhat: np.ndarray, dtrain: \"xgb.DMatrix\") -&gt; np.ndarray:\n        \"\"\"Compute the log cosh gradient.\n\n        Args:\n            yhat (np.array): Predictions\n            dtrain: The XGBoost / LightGBM dataset\n\n        Returns:\n            log cosh gradient\n        \"\"\"\n\n        y = dtrain.get_label()\n        return -np.tanh(y - yhat)\n\n    def _hessian(yhat: np.ndarray, dtrain: \"xgb.DMatrix\") -&gt; np.ndarray:\n        \"\"\"Compute the log cosh hessian.\n\n        Args:\n            yhat (np.array): Predictions\n            dtrain: The XGBoost / LightGBM dataset\n\n        Returns:\n            log cosh Hessian\n        \"\"\"\n\n        y = dtrain.get_label()\n        return 1. / np.power(np.cosh(y - yhat), 2)\n\n    def log_cosh_loss(\n            yhat: np.ndarray,\n            dtrain: \"xgb.DMatrix\"\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Calculate gradient and hessian for log cosh loss.\n\n        Args:\n            yhat (np.array): Predictions\n            dtrain: The XGBoost / LightGBM dataset\n\n        Returns:\n            grad: log cosh loss gradient\n            hess: log cosh loss Hessian\n        \"\"\"\n        grad = _gradient(yhat, dtrain)\n\n        hess = _hessian(yhat, dtrain)\n\n        return grad, hess\n\n    return log_cosh_loss\n</code></pre>"},{"location":"reference/loss_functions_regression.html#bokbokbok.loss_functions.regression.regression_loss_functions.SPELoss","title":"<code>SPELoss()</code>","text":"<p>Squared Percentage Error loss</p> Source code in <code>bokbokbok/loss_functions/regression/regression_loss_functions.py</code> <pre><code>def SPELoss() -&gt; Callable:\n    \"\"\"\n    Squared Percentage Error loss\n    \"\"\"\n\n    def _gradient(yhat: np.ndarray, dtrain: \"xgb.DMatrix\") -&gt; np.ndarray:\n        \"\"\"\n        Compute the gradient squared percentage error.\n        Args:\n            yhat (np.array): Predictions\n            dtrain: The XGBoost / LightGBM dataset\n\n        Returns:\n            SPE Gradient\n        \"\"\"\n        y = dtrain.get_label()\n        return -2*(y-yhat)/(y**2)\n\n    def _hessian(dtrain: \"xgb.DMatrix\") -&gt; np.ndarray:\n        \"\"\"\n        Compute the hessian for squared percentage error.\n        Args:\n            yhat (np.array): Predictions\n            dtrain: The XGBoost / LightGBM dataset\n\n        Returns:\n            SPE Hessian\n        \"\"\"\n        y = dtrain.get_label()\n        return 2/(y**2)\n\n    def squared_percentage(\n        yhat: np.ndarray, \n        dtrain: \"xgb.DMatrix\"\n        ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Calculate gradient and hessian for squared percentage error.\n\n        Args:\n            yhat (np.array): Predictions\n            dtrain: The XGBoost / LightGBM dataset\n\n        Returns:\n            grad: SPE loss gradient\n            hess: SPE loss Hessian\n        \"\"\"\n        grad = _gradient(yhat, dtrain)\n\n        hess = _hessian(dtrain)\n\n        return grad, hess\n\n    return squared_percentage\n</code></pre>"},{"location":"tutorials/F1_score.html","title":"Use F1 Score","text":"<pre><code>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom bokbokbok.eval_metrics.classification import F1_Score_Binary\nfrom bokbokbok.utils import clip_sigmoid\n\nX, y = make_classification(n_samples=1000, \n                           n_features=10, \n                           random_state=41114)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, \n                                                      y, \n                                                      test_size=0.25, \n                                                      random_state=41114)\n</code></pre> <pre><code>import lightgbm as lgb\n\ntrain = lgb.Dataset(X_train, y_train)\nvalid = lgb.Dataset(X_valid, y_valid, reference=train)\nparams = {\n     'n_estimators': 300,\n     'objective': 'binary',\n     'seed': 41114,\n     'n_jobs': 8,\n     'learning_rate': 0.1,\n     'early_stopping_rounds': 100,\n   }\n\nclf = lgb.train(params=params,\n                train_set=train,\n                valid_sets=[train, valid],\n                valid_names=['train','valid'],\n                feval=F1_Score_Binary(average='micro'))\n\nroc_auc_score(y_valid, clf.predict(X_valid))\n</code></pre> <pre><code>import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, y_train)\ndvalid = xgb.DMatrix(X_valid, y_valid)\n\nparams = {\n     'seed': 41114,\n     'objective':'binary:logistic',\n     'learning_rate': 0.1,\n    'disable_default_eval_metric': 1,\n   }\n\nbst = xgb.train(params,\n          dtrain=dtrain,\n          num_boost_round=300,\n          early_stopping_rounds=10,\n          verbose_eval=10,\n          maximize=True,\n          feval=F1_Score_Binary(average='micro', XGBoost=True),\n          evals=[(dtrain, 'dtrain'), (dvalid, 'dvalid')])\n\nroc_auc_score(y_valid, clip_sigmoid(bst.predict(dvalid)))\n</code></pre>"},{"location":"tutorials/F1_score.html#when-to-use-f-score","title":"When to use F-Score","text":"<p>The F-score or F-measure is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of true positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of true positive results divided by the number of all samples that should have been identified as positive. Precision is also known as positive predictive value, and recall is also known as sensitivity in diagnostic binary classification.</p> <p>The highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if either the precision or the recall is zero. </p>"},{"location":"tutorials/F1_score.html#usage-in-lightgbm","title":"Usage in LightGBM","text":""},{"location":"tutorials/F1_score.html#usage-in-xgboost","title":"Usage in XGBoost","text":""},{"location":"tutorials/RMSPE.html","title":"Use Root Mean Squared Percentage Error","text":"<pre><code>from sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom bokbokbok.eval_metrics.regression import RMSPEMetric\nfrom bokbokbok.loss_functions.regression import SPELoss\n\nX, y = make_regression(n_samples=10000, \n                       n_features=10, \n                       random_state=41114)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, \n                                                      y, \n                                                      test_size=0.25, \n                                                      random_state=41114)\n</code></pre> <pre><code>import lightgbm as lgb\n\ntrain = lgb.Dataset(X_train, y_train)\nvalid = lgb.Dataset(X_valid, y_valid, reference=train)\nparams = {\n     'n_estimators': 3000,\n     'seed': 41114,\n     'n_jobs': 8,\n     'max_leaves':10,\n     'early_stopping_rounds': 100,\n     'objective': SPELoss(),\n   }\n\nclf = lgb.train(params=params,\n                train_set=train,\n                valid_sets=[train, valid],\n                valid_names=['train','valid'],\n                feval=RMSPEMetric())\n\nmean_absolute_error(y_valid, clf.predict(X_valid))\n</code></pre> <pre><code>import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, y_train)\ndvalid = xgb.DMatrix(X_valid, y_valid)\n\nparams = {\n     'seed': 41114,\n     'learning_rate': 0.1,\n    'disable_default_eval_metric': 1\n   }\n\nbst = xgb.train(params,\n          dtrain=dtrain,\n          num_boost_round=3000,\n          early_stopping_rounds=100,\n          verbose_eval=100,\n          obj=SPELoss(),\n          maximize=False,\n          feval=RMSPEMetric(XGBoost=True),\n          evals=[(dtrain, 'dtrain'), (dvalid, 'dvalid')])\n\nmean_absolute_error(y_valid, bst.predict(dvalid))\n</code></pre>"},{"location":"tutorials/RMSPE.html#when-to-use-root-mean-squared-percentage-error","title":"When to use (Root Mean) Squared Percentage Error?","text":"<p>This function is defined according to this Kaggle competition for volatility calculation. </p> <p>RMSPE cannot be used as a Loss function - the gradient is constant and hence the Hessian is 0. Nevertheless, it can still be used as an evaluation metric as the model trains. To use the loss function, we simply remove the square for a non-zero Hessian.</p>"},{"location":"tutorials/RMSPE.html#usage-in-lightgbm","title":"Usage in LightGBM","text":""},{"location":"tutorials/RMSPE.html#usage-in-xgboost","title":"Usage in XGBoost","text":""},{"location":"tutorials/focal_loss.html","title":"Use Weighted Focal Loss","text":"<pre><code>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom bokbokbok.loss_functions.classification import WeightedFocalLoss\nfrom bokbokbok.eval_metrics.classification import WeightedFocalMetric\nfrom bokbokbok.utils import clip_sigmoid\n\nX, y = make_classification(n_samples=1000, \n                           n_features=10, \n                           random_state=41114)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, \n                                                      y, \n                                                      test_size=0.25, \n                                                      random_state=41114)\n\nalpha = 0.7  # Reduce False Positives\ngamma = 2    # Focus on misclassified examples more strictly\n</code></pre> <pre><code>import lightgbm as lgb\n\ntrain = lgb.Dataset(X_train, y_train)\nvalid = lgb.Dataset(X_valid, y_valid, reference=train)\nparams = {\n     'n_estimators': 300,\n     'seed': 41114,\n     'n_jobs': 8,\n     'learning_rate': 0.1,\n     'early_stopping_rounds': 100,\n     'objective': WeightedFocalLoss(alpha=alpha, gamma=gamma)\n   }\n\nclf = lgb.train(params=params,\n                train_set=train,\n                valid_sets=[train, valid],\n                valid_names=['train','valid'],\n                feval=WeightedFocalMetric(alpha=alpha, gamma=gamma)\n                )\n\nroc_auc_score(y_valid, clip_sigmoid(clf.predict(X_valid)))\n</code></pre> <pre><code>import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, y_train)\ndvalid = xgb.DMatrix(X_valid, y_valid)\n\nparams = {\n     'seed': 41114,\n     'learning_rate': 0.1,\n    'disable_default_eval_metric': 1\n   }\n\nbst = xgb.train(params,\n          dtrain=dtrain,\n          num_boost_round=300,\n          early_stopping_rounds=10,\n          verbose_eval=10,\n          obj=WeightedFocalLoss(alpha=alpha, gamma=gamma),\n          maximize=False,\n          feval=WeightedFocalMetric(alpha=alpha, gamma=gamma, XGBoost=True),\n          evals=[(dtrain, 'dtrain'), (dvalid, 'dvalid')])\n\nroc_auc_score(y_valid, clip_sigmoid(bst.predict(dvalid)))\n</code></pre>"},{"location":"tutorials/focal_loss.html#when-to-use-focal-loss","title":"When to use Focal Loss?","text":"<p>Focal Loss addresses class imbalance in tasks such as object detection. Focal loss applies a modulating term to the Cross Entropy loss in order to focus learning on hard negative examples. It is a dynamically scaled Cross Entropy loss, where the scaling factor decays to zero as confidence in the correct class increases. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. This scaling factor is gamma. The more gamma is increased, the more the model is focussed on the hard, misclassified examples.</p> <p>We employ Weighted Focal Loss, which further allows us to reduce false positives or false negatives depending on our value of alpha:</p> <p>A value alpha &gt; 1 decreases the false negative count, hence increasing the recall. Conversely, setting alpha &lt; 1 decreases the false positive count and increases the precision. </p>"},{"location":"tutorials/focal_loss.html#usage-in-lightgbm","title":"Usage in LightGBM","text":""},{"location":"tutorials/focal_loss.html#usage-in-xgboost","title":"Usage in XGBoost","text":""},{"location":"tutorials/log_cosh_loss.html","title":"Use Log Cosh Score","text":"<pre><code>from sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom bokbokbok.eval_metrics.regression import LogCoshMetric\nfrom bokbokbok.loss_functions.regression import LogCoshLoss\n\nX, y = make_regression(n_samples=1000, \n                       n_features=10, \n                       random_state=41114)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, \n                                                      y/100, \n                                                      test_size=0.25, \n                                                      random_state=41114)\n</code></pre> <pre><code>import lightgbm as lgb\n\ntrain = lgb.Dataset(X_train, y_train)\nvalid = lgb.Dataset(X_valid, y_valid, reference=train)\nparams = {\n     'n_estimators': 3000,\n     'seed': 41114,\n     'n_jobs': 8,\n     'learning_rate': 0.1,\n     'verbose': 100,\n     'early_stopping_rounds': 100,\n     'objective': LogCoshLoss()\n   }\n\nclf = lgb.train(params=params,\n                train_set=train,\n                valid_sets=[train, valid],\n                valid_names=['train','valid'],\n                feval=LogCoshMetric(),\n                verbose_eval=100)\n\nmean_absolute_error(y_valid, clf.predict(X_valid))\n</code></pre> <pre><code>import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, y_train)\ndvalid = xgb.DMatrix(X_valid, y_valid)\n\nparams = {\n     'seed': 41114,\n     'learning_rate': 0.1,\n    'disable_default_eval_metric': 1\n   }\n\nbst = xgb.train(params,\n          dtrain=dtrain,\n          num_boost_round=3000,\n          early_stopping_rounds=10,\n          verbose_eval=100,\n          obj=LogCoshLoss(),\n          maximize=False,\n          feval=LogCoshMetric(XGBoost=True),\n          evals=[(dtrain, 'dtrain'), (dvalid, 'dvalid')])\n\nmean_absolute_error(y_valid, bst.predict(dvalid))\n</code></pre>"},{"location":"tutorials/log_cosh_loss.html#when-to-use-log-cosh-loss","title":"When to use Log Cosh Loss?","text":"<p>Log Cosh Loss addresses the small number of problems that can arise from using Mean Absolute Error due to its sharpness. Log(cosh(x)) is a way to very closely approximate Mean Absolute Error while retaining a 'smooth' function.</p> <p>Do note that large y-values can cause issues here, which is why the y-values are scaled below</p>"},{"location":"tutorials/log_cosh_loss.html#usage-in-lightgbm","title":"Usage in LightGBM","text":""},{"location":"tutorials/log_cosh_loss.html#usage-in-xgboost","title":"Usage in XGBoost","text":""},{"location":"tutorials/quadratic_weighted_kappa.html","title":"Use Quadratic Weighted Kappa","text":"<pre><code>from sklearn.datasets import make_multilabel_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nfrom bokbokbok.eval_metrics.classification import QuadraticWeightedKappaMetric\nfrom bokbokbok.utils import clip_sigmoid\n\nX, y = make_multilabel_classification(n_samples=1000, \n                                      n_features=10,\n                                      n_classes=2,\n                                      n_labels=1,\n                                      random_state=41114)\ny = y.sum(axis=1)\nX_train, X_valid, y_train, y_valid = train_test_split(X, \n                                                      y, \n                                                      test_size=0.25, \n                                                      random_state=41114)\n</code></pre> <pre><code>import lightgbm as lgb\n\ntrain = lgb.Dataset(X_train, y_train)\nvalid = lgb.Dataset(X_valid, y_valid, reference=train)\nparams = {\n     'n_estimators': 300,\n     'seed': 41114,\n     'n_jobs': 8,\n     'learning_rate': 0.1,\n    'objective':'multiclass',\n    'num_class': 3,\n    'early_stopping_rounds': 100,\n   }\n\nclf = lgb.train(params=params,\n                train_set=train,\n                valid_sets=[train, valid],\n                valid_names=['train','valid'],\n                feval=QuadraticWeightedKappaMetric())\n</code></pre> <pre><code>import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, y_train)\ndvalid = xgb.DMatrix(X_valid, y_valid)\n\nparams = {\n     'seed': 41114,\n     'learning_rate': 0.1,\n    'disable_default_eval_metric': 1,\n    'objective': 'multi:softprob',\n    'num_class': 3\n   }\n\nbst = xgb.train(params,\n          dtrain=dtrain,\n          num_boost_round=300,\n          early_stopping_rounds=100,\n          verbose_eval=10,\n          feval=QuadraticWeightedKappaMetric(XGBoost=True),\n          maximize=True,\n          evals=[(dtrain, 'dtrain'), (dvalid, 'dvalid')])\n</code></pre>"},{"location":"tutorials/quadratic_weighted_kappa.html#usage-in-lightgbm","title":"Usage in LightGBM","text":""},{"location":"tutorials/quadratic_weighted_kappa.html#usage-in-xgboost","title":"Usage in XGBoost","text":""},{"location":"tutorials/weighted_cross_entropy.html","title":"Use Weighted Cross Entropy","text":"<pre><code>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom bokbokbok.loss_functions.classification import WeightedCrossEntropyLoss\nfrom bokbokbok.eval_metrics.classification import WeightedCrossEntropyMetric\nfrom bokbokbok.utils import clip_sigmoid\n\nX, y = make_classification(n_samples=1000, \n                           n_features=10, \n                           random_state=41114)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, \n                                                      y, \n                                                      test_size=0.25, \n                                                      random_state=41114)\n\nalpha = 0.7  # Reduce False Positives\n</code></pre> <pre><code>import lightgbm as lgb\n\ntrain = lgb.Dataset(X_train, y_train)\nvalid = lgb.Dataset(X_valid, y_valid, reference=train)\nparams = {\n     'n_estimators': 300,\n     'seed': 41114,\n     'n_jobs': 8,\n     'learning_rate': 0.1,\n     'early_stopping_rounds': 100,\n     'objective': WeightedCrossEntropyLoss(alpha=alpha),\n   }\n\nclf = lgb.train(params=params,\n                train_set=train,\n                valid_sets=[train, valid],\n                valid_names=['train','valid'],\n                feval=WeightedCrossEntropyMetric(alpha=alpha))\n\nroc_auc_score(y_valid, clip_sigmoid(clf.predict(X_valid)))\n</code></pre> <pre><code>import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, y_train)\ndvalid = xgb.DMatrix(X_valid, y_valid)\n\nparams = {\n     'seed': 41114,\n     'learning_rate': 0.1,\n    'disable_default_eval_metric': 1\n   }\n\nbst = xgb.train(params,\n          dtrain=dtrain,\n          num_boost_round=300,\n          early_stopping_rounds=10,\n          verbose_eval=10,\n          obj=WeightedCrossEntropyLoss(alpha=alpha),\n          maximize=False,\n          feval=WeightedCrossEntropyMetric(alpha=alpha, XGBoost=True),\n          evals=[(dtrain, 'dtrain'), (dvalid, 'dvalid')])\n\nroc_auc_score(y_valid, clip_sigmoid(bst.predict(dvalid)))\n</code></pre>"},{"location":"tutorials/weighted_cross_entropy.html#when-to-use-weighted-cross-entropy","title":"When to use Weighted Cross Entropy?","text":"<p>A factor alpha is added in to Cross Entropy, allowing one to trade off recall and precision by up- or down-weighting the cost of a positive error relative to a negative error.</p> <p>A value alpha &gt; 1 decreases the false negative count, hence increasing the recall. Conversely, setting alpha &lt; 1 decreases the false positive count and increases the precision. </p>"},{"location":"tutorials/weighted_cross_entropy.html#usage-in-lightgbm","title":"Usage in LightGBM","text":""},{"location":"tutorials/weighted_cross_entropy.html#usage-in-xgboost","title":"Usage in XGBoost","text":""}]}